{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers[sentencepiece] in /opt/anaconda3/lib/python3.8/site-packages (4.44.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (3.0.12)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.25.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (1.24.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (5.3.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (2020.11.13)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (2.25.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (4.54.1)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (5.28.1)\n",
      "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /opt/anaconda3/lib/python3.8/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (2024.9.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[sentencepiece]) (4.8.0)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (2.10)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (1.26.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->transformers[sentencepiece]) (2023.5.7)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install 'transformers[sentencepiece]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ea232e7a52742e8a08ff57799968fb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='config.json'), FloatProgress(value=0.0, max=570.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c1c36ed4e4148d5b20725b82967c554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='model.safetensors'), FloatProgress(value=0.0, max=440449768.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The number of parameters in bert-base-uncased: 109482240\n",
      "Estimated memory required for inference: 0.41 GB\n",
      "Estimated memory required for GPT-3 (175B parameters): 651.93 GB\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "def compute_model_size(model_checkpoint='bert-base-uncased'):\n",
    "    '''\n",
    "    This function calculates the total number of parameters in a given NLP model.\n",
    "    \n",
    "    Parameters:\n",
    "        model_checkpoint: A string representing the model checkpoint from Hugging Face's model hub (default is BERT base uncased).\n",
    "        \n",
    "    Returns:\n",
    "        total_params: The total number of parameters in the model.\n",
    "    '''\n",
    "    \n",
    "    model = AutoModel.from_pretrained(model_checkpoint)\n",
    "    total_params = sum(param.numel() for param in model.parameters())\n",
    "    return total_params\n",
    "\n",
    "def estimate_memory_requirement(parameter_count):\n",
    "    '''\n",
    "    This function estimates the memory required to run inference on a model.\n",
    "    \n",
    "    Each parameter is stored as a single-precision floating point (32 bits = 4 bytes).\n",
    "    \n",
    "    Parameters:\n",
    "        parameter_count: The total number of parameters in the model.\n",
    "        \n",
    "    Returns:\n",
    "        memory_in_gb: The estimated memory required in gigabytes.\n",
    "    '''\n",
    "    \n",
    "    bytes_per_param = 4  # Single precision floating point in bytes\n",
    "    total_memory_in_bytes = parameter_count * bytes_per_param\n",
    "    memory_in_gb = total_memory_in_bytes / (1024 ** 3)  # Convert from bytes to gigabytes\n",
    "    return memory_in_gb\n",
    "\n",
    "# Define the BERT model checkpoint to explore\n",
    "model_checkpoint = 'bert-base-uncased'\n",
    "\n",
    "# Compute the number of parameters in the model\n",
    "num_params = compute_model_size(model_checkpoint)\n",
    "print(f\"The number of parameters in {model_checkpoint}: {num_params}\")\n",
    "\n",
    "# Estimate the memory required for inference\n",
    "memory_gb = estimate_memory_requirement(num_params)\n",
    "print(f\"Estimated memory required for inference: {memory_gb:.2f} GB\")\n",
    "\n",
    "# Example for GPT-3's largest model (175 billion parameters)\n",
    "gpt3_params = 175e9\n",
    "gpt3_memory_gb = estimate_memory_requirement(gpt3_params)\n",
    "print(f\"Estimated memory required for GPT-3 (175B parameters): {gpt3_memory_gb:.2f} GB\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
